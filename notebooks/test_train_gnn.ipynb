{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0d5b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/naodell/work/hgcal/hgcal_gravnet_model\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/home/naodell/opt/miniconda3/envs/hgcal-ml/lib/python3.9/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4159608/1549542191.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#from torch.utils.tensorboard import SummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjectcondensation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mobject_condensation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/hgcal-ml/lib/python3.9/site-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/hgcal-ml/lib/python3.9/site-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhetero_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/hgcal-ml/lib/python3.9/site-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m from typing import (Optional, Dict, Any, Union, List, Iterable, Tuple,\n\u001b[1;32m      2\u001b[0m                     NamedTuple, Callable)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/hgcal-ml/lib/python3.9/site-packages/torch_geometric/typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Types for accessing data ####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/hgcal-ml/lib/python3.9/site-packages/torch_sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;34m'_relabel'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m ]:\n\u001b[0;32m---> 15\u001b[0;31m     torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n\u001b[0m\u001b[1;32m     16\u001b[0m         f'{library}_{suffix}', [osp.dirname(__file__)]).origin)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/hgcal-ml/lib/python3.9/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/hgcal-ml/lib/python3.9/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: /home/naodell/opt/miniconda3/envs/hgcal-ml/lib/python3.9/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs"
     ]
    }
   ],
   "source": [
    "%cd /home/naodell/work/hgcal/hgcal_gravnet_model\n",
    "\n",
    "import os, os.path as osp\n",
    "from time import strftime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import scripts.objectcondensation as object_condensation\n",
    "from torch_cmspepr.gravnet_model import GravnetModel\n",
    "from torch_cmspepr.dataset import TauDataset\n",
    "from scripts.lrscheduler import CyclicLRWithRestarts\n",
    "from scripts.nadam import Nadam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global options \n",
    "\n",
    "object_condensation.DEBUG = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')       \n",
    "print('Using device', device)                                               \n",
    "\n",
    "reduced_data = False\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "data_path = 'local_data/npzs_all'\n",
    "dataset = TauDataset(data_path)\n",
    "dataset.blacklist([ # Remove a bunch of bad events                          \n",
    "    f'{data_path}/110_nanoML_98.npz',                                          \n",
    "    f'{data_path}/113_nanoML_13.npz',                                          \n",
    "    f'{data_path}/124_nanoML_77.npz',                                          \n",
    "    f'{data_path}/128_nanoML_70.npz',                                          \n",
    "    f'{data_path}/149_nanoML_90.npz',                                          \n",
    "    f'{data_path}/153_nanoML_22.npz',                                          \n",
    "    f'{data_path}/26_nanoML_93.npz',                                           \n",
    "    f'{data_path}/32_nanoML_45.npz',                                           \n",
    "    f'{data_path}/5_nanoML_51.npz',                                            \n",
    "    f'{data_path}/86_nanoML_97.npz',                                           \n",
    "    ])                                                                      \n",
    "\n",
    "if reduced_data:\n",
    "    keep = .01\n",
    "    print(f'Keeping only {100.*keep:.1f}% of events for debugging')\n",
    "    dataset, _ = dataset.split(keep)\n",
    "    \n",
    "shuffle = True                                                              \n",
    "train_dataset, test_dataset = dataset.split(.8)                             \n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size = batch_size, \n",
    "                          shuffle = shuffle\n",
    "                         )\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size = batch_size, \n",
    "                         shuffle = shuffle\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and configure it\n",
    "\n",
    "model = GravnetModel(input_dim=9, output_dim=4).to(device)\n",
    "                                                                            \n",
    "epoch_size = len(train_loader.dataset)                                      \n",
    "#optimizer = Nadam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "# scheduler = CyclicLRWithRestarts(optimizer, batch_size, epoch_size, restart_period=400, t_mult=1.1, policy=\"cosine\")\n",
    "                                                                            \n",
    "loss_offset = 1. # To prevent a negative loss from ever occuring            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss\n",
    "\n",
    "def loss_fn(out, data, s_c=1., return_components=False):                    \n",
    "    device = out.device                                                     \n",
    "    pred_betas = torch.sigmoid(out[:,0])                                    \n",
    "    pred_cluster_space_coords = out[:,1:4]                                  \n",
    "    # pred_cluster_properties = out[:,3:]                                   \n",
    "    assert all(t.device == device for t in [pred_betas, pred_cluster_space_coords, data.y, data.batch,                                                         \n",
    "        # pred_cluster_properties, data.truth_cluster_props                 \n",
    "        ])                                                                  \n",
    "    \n",
    "    out_oc = object_condensation.calc_LV_Lbeta(                              \n",
    "        pred_betas,                                                         \n",
    "        pred_cluster_space_coords,                                          \n",
    "        data.y.long(),                                                      \n",
    "        data.batch,                                                         \n",
    "        return_components = return_components                                 \n",
    "        )                                                                   \n",
    "    \n",
    "    if return_components:                                                   \n",
    "        return out_oc                                                       \n",
    "    else:                                                                   \n",
    "        LV, Lbeta = out_oc                                                  \n",
    "        return LV + Lbeta + loss_offset                                     \n",
    "    \n",
    "    # Lp = objectcondensation.calc_Lp(                                                                                                                                                                                                                  \n",
    "    #     pred_betas,                                                       \n",
    "    #     data.y.long(),                                                    \n",
    "    #     pred_cluster_properties,                                          \n",
    "    #     data.truth_cluster_props                                          \n",
    "    #     )                                                                 \n",
    "    # return Lp + s_c*(LV + Lbeta)                                          \n",
    "                                                                                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bea4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and test methods\n",
    "\n",
    "def train(epoch, train_loader, tb_writer):\n",
    "    #print('Training epoch', epoch)\n",
    "    model.train()\n",
    "    # scheduler.step()\n",
    "    try:\n",
    "        pbar = tqdm(train_loader, total=len(train_loader), leave=False)\n",
    "        pbar.set_postfix({'loss': '?'})\n",
    "        for i, data in enumerate(pbar):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            result = model(data.x, data.batch)\n",
    "            loss = loss_fn(result, data)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.batch_step()\n",
    "            pbar.set_postfix({'loss': float(loss)})\n",
    "        \n",
    "            tb_writer.add_scalar('training loss', loss, i)\n",
    "        return loss\n",
    "            \n",
    "    except Exception:\n",
    "        print('Exception encountered:', data, ', npzs:')\n",
    "        print('  ' + '\\n  '.join([train_dataset.npzs[int(i)] for i in data.inpz]))\n",
    "        raise\n",
    "\n",
    "def test(epoch):\n",
    "    N_test = len(test_loader)\n",
    "    loss_components = {}\n",
    "    \n",
    "    def update(components):\n",
    "        for key, value in components.items():\n",
    "            if not key in loss_components: loss_components[key] = 0.\n",
    "            loss_components[key] += value\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in tqdm(test_loader, total=len(test_loader), leave=False):\n",
    "            data = data.to(device)\n",
    "            result = model(data.x, data.batch)\n",
    "            update(loss_fn(result, data, return_components=True))\n",
    "            \n",
    "    # Divide by number of entries\n",
    "    for key in loss_components:\n",
    "        loss_components[key] /= N_test\n",
    "        \n",
    "    # Compute total loss and do printout\n",
    "    test_loss = loss_offset + loss_components['L_V']+loss_components['L_beta']\n",
    "    \n",
    "    #print('test ' + object_condensation.formatted_loss_components_string(loss_components))\n",
    "    #print(f'Returning {test_loss}')\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e33f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional useful utility functions\n",
    "\n",
    "def write_checkpoint(checkpoint_number=None, best=False):\n",
    "    ckpt_dir = strftime('checkpoints/gravnet_%b%d_%H%M') #if args.ckptdir is None else args.ckptdir\n",
    "    ckpt = 'ckpt_best.pth.tar' if best else 'ckpt_{0}.pth.tar'.format(checkpoint_number)\n",
    "    ckpt = osp.join(ckpt_dir, ckpt)\n",
    "    if best: \n",
    "        print('Saving epoch {0} as new best'.format(checkpoint_number))\n",
    "    \n",
    "    if not reduced_data:\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        torch.save(dict(model=model.state_dict()), ckpt)\n",
    "\n",
    "def debug():\n",
    "    oc.DEBUG = True\n",
    "    dataset = TauDataset('data/taus')\n",
    "    dataset.npzs = [\n",
    "        # 'data/taus/49_nanoML_84.npz',\n",
    "        # 'data/taus/37_nanoML_4.npz',\n",
    "        'data/taus/26_nanoML_93.npz',\n",
    "        # 'data/taus/142_nanoML_75.npz',\n",
    "        ]\n",
    "    for data in DataLoader(dataset, batch_size=len(dataset), shuffle=False): break\n",
    "    \n",
    "    print(data.y.sum())\n",
    "    model = GravnetModel(input_dim=9, output_dim=4)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(data.x, data.batch)\n",
    "        \n",
    "    pred_betas = torch.sigmoid(out[:,0])\n",
    "    pred_cluster_space_coords = out[:,1:4]\n",
    "    out_oc = oc.calc_LV_Lbeta(\n",
    "        pred_betas,\n",
    "        pred_cluster_space_coords,\n",
    "        data.y.long(),\n",
    "        data.batch.long()\n",
    "        )\n",
    "\n",
    "def run_profile():\n",
    "    from torch.profiler import profile, record_function, ProfilerActivity\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device', device)\n",
    "\n",
    "    batch_size = 2\n",
    "    n_batches = 2\n",
    "    shuffle = True\n",
    "    dataset = TauDataset('data/taus')\n",
    "    dataset.npzs = dataset.npzs[:batch_size*n_batches]\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    print(f'Running profiling for {len(dataset)} events, batch_size={batch_size}, {len(loader)} batches')\n",
    "\n",
    "    model = GravnetModel(input_dim=9, output_dim=8).to(device)\n",
    "    epoch_size = len(loader.dataset)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-7, weight_decay=1e-4)\n",
    "\n",
    "    print('Start limited training loop')\n",
    "    model.train()\n",
    "    with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            pbar = tqdm(loader, total=len(loader), leave=False)\n",
    "            pbar.set_postfix({'loss': '?'})\n",
    "            for i, data in enumerate(pbar):\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                result = model(data.x, data.batch)\n",
    "                loss = loss_fn(result, data)\n",
    "                print(f'loss={float(loss)}')\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar.set_postfix({'loss': float(loss)})\n",
    "                \n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time\", row_limit=10))\n",
    "    # Other valid keys:\n",
    "    # cpu_time, cuda_time, cpu_time_total, cuda_time_total, cpu_memory_usage,\n",
    "    # cuda_memory_usage, self_cpu_memory_usage, self_cuda_memory_usage, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18059063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d03aa36e424adba5fd15feb9ad3686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3978 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4110c327c4d5478e81101595c419068a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "min_loss = 1e9\n",
    "n_epochs = 50\n",
    "tb_writer = SummaryWriter(log_dir='logs')\n",
    "for i_epoch in tqdm(range(n_epochs)):\n",
    "    train_loss = train(i_epoch, train_loader, tb_writer)\n",
    "    write_checkpoint(i_epoch)\n",
    "    test_loss = test(i_epoch)\n",
    "    if test_loss < min_loss:\n",
    "        min_loss = test_loss\n",
    "        write_checkpoint(i_epoch, best=True)\n",
    "    \n",
    "    tb_writer.add_scalar('test loss', test_loss, i_epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b727ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdfdb05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
